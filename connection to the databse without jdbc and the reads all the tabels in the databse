# Importing required libraries for the Glue job
import sys  # For handling system-specific parameters and arguments
import logging  # For logging info and error messages
from pyspark.context import SparkContext  # SparkContext to establish a Spark session
from pyspark.sql import SparkSession  # SparkSession to work with Spark DataFrames
from awsglue.context import GlueContext  # GlueContext for AWS Glue operations
from awsglue.dynamicframe import DynamicFrame  # DynamicFrame for working with AWS Glue DataFrames
from awsglue.utils import getResolvedOptions  # To fetch job parameters passed during execution

# Setting up logging to capture info and errors during execution
logging.basicConfig(level=logging.INFO)  # Setting logging level to INFO
logger = logging.getLogger(__name__)  # Get the logger for the script

# Fetching arguments passed to the script (like database credentials and S3 output path)
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'DATABASE_NAME', 'DB_HOST', 'DB_PORT', 'DB_USER', 'DB_PASSWORD', 'S3_OUTPUT_PATH'])

# Assigning values from arguments to variables for easier use
database_name = args['DATABASE_NAME']  # Database name in the Glue catalog
db_host = args['DB_HOST']  # Hostname or IP address of the database
db_port = args['DB_PORT']  # Port on which the database is running
db_user = args['DB_USER']  # Username for database authentication
db_password = args['DB_PASSWORD']  # Password for database authentication
s3_output_path = args['S3_OUTPUT_PATH']  # S3 path to store extracted data

# Creating a SparkContext to initialize Spark
sc = SparkContext()
spark = SparkSession(sc)

# Creating a GlueContext for Glue-specific operations
glue_context = GlueContext(sc)

# Function to extract all table names from the database (without JDBC)
def get_all_tables_from_database():
    """
    Fetches all table names from the specified database without using JDBC (direct connection).
    """
    try:
        # Setting up connection details for the database using PySpark (Direct DB connection)
        # This is done by using the Spark `read.format("jdbc")` functionality to connect without the AWS Glue JDBC connection
        connection_options = {
            "url": f"jdbc:mysql://{db_host}:{db_port}/{database_name}",  # JDBC URL (you can customize based on your DB type)
            "user": db_user,  # Username for database authentication
            "password": db_password,  # Password for database authentication
            "dbtable": "information_schema.tables",  # This query fetches the list of tables in the database
        }
        
        # Reading metadata to get table names
        table_metadata_df = spark.read.format("jdbc").options(**connection_options).load()

        # Extracting table names from the DataFrame
        table_names = table_metadata_df.select("table_name").rdd.flatMap(lambda x: x).collect()
        
        logger.info(f"Fetched tables: {table_names}")
        return table_names
    except Exception as e:
        logger.error(f"Error fetching table names: {e}")
        sys.exit(1)

# Function to extract and store data from each table and save to S3
def extract_and_store_data(table_name):
    """
    Extracts data from the given table and saves it to S3.
    """
    try:
        # Use GlueContext to create a DynamicFrame from the specified table
        dynamic_frame = glue_context.create_dynamic_frame.from_options(
            connection_type="mysql",  # Set connection type for MySQL (you can modify this for other DB types)
            connection_options={
                "url": f"jdbc:mysql://{db_host}:{db_port}/{database_name}",  # JDBC URL
                "user": db_user,  # Username
                "password": db_password,  # Password
                "dbtable": table_name,  # Table name to read
            },
            transformation_ctx=f"dynamic_frame_{table_name}"  # Transformation context to keep track of the process
        )

        # Convert DynamicFrame to DataFrame for processing (optional but recommended for some operations)
        dataframe = dynamic_frame.toDF()

        # Construct the S3 path to save the data, creating a folder named after the table
        output_path = f"{s3_output_path}/{table_name}/"

        # Write the DataFrame as a CSV file to S3
        dataframe.write.format("csv").option("header", "true").save(output_path)

        logger.info(f"Data from table {table_name} saved to {output_path}")
    except Exception as e:
        logger.error(f"Error extracting and storing data for table {table_name}: {e}")

# Main function to run the entire process
def main():
    # Step 1: Fetch all table names from the database
    table_names = get_all_tables_from_database()
    
    # Step 2: Extract data from each table and store it in S3
    for table_name in table_names:
        extract_and_store_data(table_name)

# Start the Glue job by calling the main function
if __name__ == "__main__":
    main()
